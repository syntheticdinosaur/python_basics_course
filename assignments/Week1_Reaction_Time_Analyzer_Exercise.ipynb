{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧠 Week 1 – Reaction Time Analyzer (Solution)\n",
        "\n",
        "This notebook contains the **full instructor version** of the Week 1 seminar assignment. It demonstrates how to perform simple data analysis and sanity checks on simulated **reaction time (RT)** data.\n",
        "\n",
        "In behavioral and cognitive neuroscience, reaction times are one of the most basic measurements of performance. Before interpreting such data, it’s crucial to perform some *data sanity checks*: identify outliers, compute summary statistics, and verify plausibility. This notebook shows how to do this step by step using Python basics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expected Output Example\n",
        "```\n",
        "Trials analyzed: 11\n",
        "Mean RT: 305.0 ms\n",
        "Median RT: 290 ms\n",
        "Fastest RT: 250 ms\n",
        "Slowest RT: 500 ms\n",
        "Performance: Average\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 – Define the Data\n",
        "In an experiment, each trial records a *reaction time* — how long the participant takes to respond to a stimulus. These values are collected in a list.\n",
        "\n",
        "Real datasets can contain hundreds of trials, but here we only take **12** for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reaction_times = [250, 310, 295, 280, 275, 305, 290, 265, 300, 1080, 285, 500]\n",
        "print('Reaction times:', reaction_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 – Count the Trials\n",
        "Before analyzing, always check how many trials were recorded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Count how many trials were recorded and print the result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 – Summary Statistics: Describe the Data\n",
        "We first compute **mean** and **median** reaction times to describe the dataset.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "$$\n",
        "\\text{mean} = \\frac{1}{N}\\sum_{i=1}^{N} RT_i, \\quad\n",
        "\\text{median} =\n",
        "\\begin{cases}\n",
        "RT_{\\frac{N+1}{2}}, & N \\text{ odd} \\\\\n",
        "\\frac{RT_{\\frac{N}{2}} + RT_{\\frac{N}{2}+1}}{2}, & N \\text{ even}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The **mean** is simply the average reaction time across all trials, while the **median** is the middle value — *but only after the data have been sorted in ascending order.*  \n",
        "Sorting ensures that smaller reaction times come first, so we can correctly identify the middle point.\n",
        "\n",
        "We’ll now implement these calculations manually, this helps you understand how these operations actually work internally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 Note on Median Indexing\n",
        "In mathematical formulas, list positions start at **1**, e.g. $RT_1$ is the first element.\n",
        "\n",
        "In **Python**, indices start at **0**, so we must shift every index by one:\n",
        "\n",
        "$$RT_{\\frac{N+1}{2}} (math) \\Rightarrow RT_{\\frac{N+1}{2}-1} = RT_{\\frac{N-1}{2}} (Python)$$\n",
        "\n",
        "Thus, we simply **subtract one after applying the formula** to get the correct element. Since division creates a decimal value, we wrap the result in `int()` to convert it into an integer index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort the data first\n",
        "sorted_rts = sorted(reaction_times)\n",
        "\n",
        "# TODO: compute the sum of all RTs manually\n",
        "# hint: use a for-loop and a variable to keep track of the running total\n",
        "\n",
        "# TODO: compute the mean manually and store it in the variable mean_rt\n",
        "\n",
        "# TODO: compute the median manually store it in the variable median_rt (use int() for indices)\n",
        "\n",
        "print(\"Sorted RTs:\", sorted_rts)\n",
        "print(\"Mean RT:\", mean_rt, \"ms\")\n",
        "print(\"Median RT:\", median_rt, \"ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 – Fastest and Slowest Trial\n",
        "We now scan through the list to find the **fastest** and **slowest** trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: go through all reaction times and find the fastest and slowest values\n",
        "# and store them in seperate variables\n",
        "\n",
        "\n",
        "print('Fastest RT:', fastest, 'ms')\n",
        "print('Slowest RT:', slowest, 'ms')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 – Remove Implausible Trials (Outliers)\n",
        "Some reaction times are implausible — **too short** (<150 ms) or **too long** (>1000 ms). These are considered *outliers*.\n",
        "\n",
        "There are two possible ways to handle them:\n",
        "1. **Create a new list** containing only valid reaction times (recommended for clarity).\n",
        "2. **Remove invalid values directly** using `.pop()`, but loop backwards to avoid shifting indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1 – create a new list with valid reaction times\n",
        "cleaned_rts = []\n",
        "# TODO: loop through reaction_times and append only plausible RTs\n",
        "# (between 150 and 1000 ms)\n",
        "\n",
        "print(\"Cleaned RTs (new list):\", cleaned_rts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 2 – pop elements directly (loop backwards)\n",
        "reaction_times_pop = reaction_times[:]  # copy original\n",
        "# TODO: loop backwards and remove invalid RTs\n",
        "\n",
        "print(\"Cleaned RTs (pop method):\", reaction_times_pop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the data are cleaned, **copy your code from Step 3** (for mean and median) and run it again using `cleaned_rts`. This shows how cleaning affects the summary statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 – Recalculate Summary Statistics (After Cleaning)\n",
        "Let’s repeat the same computations from Step 3 with the cleaned data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Copy your code from Step 3 and re-run it here using cleaned_rts\n",
        "# Store your results in new variables called mean_cleaned and median_cleaned\n",
        "# so you can compare before and after cleaning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7 – Classify Overall Performance\n",
        "We classify the subject’s performance based on the average reaction time:\n",
        "\n",
        "| Mean RT (ms) | Performance |\n",
        "|---------------|-------------|\n",
        "| < 280 | Excellent |\n",
        "| 280–310 | Average |\n",
        "| > 310 | Needs improvement |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: classify based on mean_cleaned value\n",
        "# < 280 -> \"Excellent\", between 280–310 -> \"Average\", > 310 -> \"Needs improvement\"\n",
        "\n",
        "if ... :\n",
        "    performance = \"...\"\n",
        "elif ... :\n",
        "    performance = \"...\"\n",
        "else:\n",
        "    performance = \"...\"\n",
        "\n",
        "print(\"Performance:\", performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8 – Print Summary of Results\n",
        "We can now print a full summary of all the key results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Trials analyzed:', len(cleaned_rts))\n",
        "print('Mean RT:', round(mean_cleaned, 1), 'ms')\n",
        "print('Median RT:', median_cleaned, 'ms')\n",
        "print('Fastest RT:', min(cleaned_rts), 'ms')\n",
        "print('Slowest RT:', max(cleaned_rts), 'ms')\n",
        "print('Performance:', performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9 – Store Results in a Dictionary\n",
        "Now that we have all results, we can place them together in a **dictionary**. This lets us keep everything organized in one variable, making it easier to access or print later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"num_trials\": ...,      # TODO\n",
        "    \"mean\": ...,            # TODO\n",
        "    \"median\": ...,          # TODO\n",
        "    \"fastest\": ...,         # TODO\n",
        "    \"slowest\": ...,         # TODO\n",
        "    \"performance\": ...      # TODO\n",
        "}\n",
        "\n",
        "print(\"Results dictionary:\")\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10 – Built-In Functions Teaser\n",
        "Python provides built-in functions that can do these steps instantly, but understanding the manual approach first is valuable. Here’s how it could look with built-ins:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a563d6dc",
      "metadata": {},
      "source": [
        "## Step 10 – Built-In Functions Teaser\n",
        "All the analysis steps above (counting trials, computing mean and median, finding fastest and slowest) can also be done in one line each using Python’s built-in functions.\n",
        "Here, we use the cleaned list of reaction times `cleaned_rts` directly as input.\n",
        "The only step we still do manually is the performance classification, since it depends on our own thresholds and interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "print('Trials analyzed:', len(cleaned_rts))\n",
        "print('Mean RT:', round(statistics.mean(cleaned_rts),1), 'ms')\n",
        "print('Median RT:', statistics.median(cleaned_rts), 'ms')\n",
        "print('Fastest RT:', min(cleaned_rts), 'ms')\n",
        "print('Slowest RT:', max(cleaned_rts), 'ms')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧩 Bonus 1 – Full Reaction Times (Larger Dataset)\n",
        "We can test if our code still works for larger datasets. The following code generates **1000 trials** with mostly realistic reaction times and a few outliers.\n",
        "\n",
        "Simply overwrite your `reaction_times` list at the top with this new dataset and rerun your analysis. If it runs without errors, your code is general and robust!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "n_trials = 1000             # number of simulated trials\n",
        "random.seed(42)             # ensures reproducibility\n",
        "\n",
        "full_reaction_times = []\n",
        "for i in range(n_trials):\n",
        "    rt = random.gauss(275, 25)         # generate RT around 300 ms\n",
        "    if random.random() < 0.01:         # add ~1% random outliers\n",
        "        rt = random.choice([random.randint(50, 120), random.randint(900, 1200)])\n",
        "    full_reaction_times.append(round(rt, 1))\n",
        "\n",
        "print('Example of first 20 reaction times:')\n",
        "print(full_reaction_times[:20])\n",
        "print('Total trials generated:', len(full_reaction_times))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚙️ Bonus 2 – Sorting Algorithm (Bubble Sort)\n",
        "Finally, we can implement our own sorting algorithm — **Bubble Sort**. This algorithm repeatedly compares two neighboring elements and swaps them if they are in the wrong order.\n",
        "\n",
        "Each pass pushes the largest remaining value toward the end of the list, like a bubble rising to the surface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# strictly not the literal bubble sort\n",
        "\n",
        "bubble_sorted = cleaned_rts[:]\n",
        "for i in range(len(bubble_sorted)-1):\n",
        "    for j in range(len(bubble_sorted)-1-i):\n",
        "        if bubble_sorted[j] > bubble_sorted[j+1]:\n",
        "            bubble_sorted[j], bubble_sorted[j+1] = bubble_sorted[j+1], bubble_sorted[j]\n",
        "print('Bubble-sorted RTs:', bubble_sorted)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mne",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
